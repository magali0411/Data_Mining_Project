{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Projet Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#import contractions\n",
    "import nltk \n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importation des datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(48 out of 278 people found this comment usefu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Went to watch this movie expecting a 'nothing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A good cast and they do their best with what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The only thing that kept me from vomiting afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I just watched this film 15 minutes ago, and I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 307,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/dataset.csv', sep='\\t', header = None, names = [\"Avis\"], nrows=10)\n",
    "labels = pd.read_csv('Data/labels.csv', sep='\\t', header = None, names = ['Note'])\n",
    "#sns.heatmap(df.isnull(), cbar=False)\n",
    "all = pd.concat([df.Avis,labels.Note], sort =True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Etape 1 : Rechercher les meilleurs pré-traitements et les meilleurs classifieurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dans cette étape vous pourrez effectuer différents prétraitements et utiliser différents classifieurs\n",
    "afin de rechercher ceux qui sont les plus efficaces. Il ne faut pas hésiter à utiliser différents\n",
    "classifieurs car un classifieur qui s’avère efficace sur un jeu de données est peut être inefficace sur\n",
    "un autre jeu de données. Attention également à l’évaluation de vos modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2) Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le temps écoulé pour ce traitement a prit :  0.13488364219665527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'act.boring',\n",
       " 'little',\n",
       " 'tale',\n",
       " 'sweet',\n",
       " 'innocent',\n",
       " 'sally',\n",
       " 'ayers',\n",
       " 'being',\n",
       " 'drugged',\n",
       " 'forced',\n",
       " 'white',\n",
       " 'slavery',\n",
       " 'prostitution',\n",
       " '.']"
      ]
     },
     "execution_count": 308,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc=df.Avis.copy()\n",
    "dfc_phrases = df.Avis.copy()\n",
    "\n",
    "debut = time.time()\n",
    "for i in range(0, len(df)-1, 1):\n",
    "    \n",
    "    dfc[i]=''.join(dfc[i]).lower() #jusqu'ici c'est une liste\n",
    "    dfc[i]=sent_tokenize(dfc[i])# ici cast de la liste en string(avis entier)\n",
    "    dfc_phrases[i] = dfc[i] # Ici on va chuck nos phrase et apposer nos tag\n",
    "\n",
    "    for j in range(0, len(dfc.loc[i])-1, 1):\n",
    "        \n",
    "        #dfc.loc[i][j]= contractions.fix(dfc.loc[i][j])\n",
    "        dfc.loc[i][j]=dfc.loc[i][j].lower()\n",
    "        dfc.loc[i][j]=word_tokenize(dfc.loc[i][j])\n",
    "        sentence = dfc.loc[i][j]\n",
    "        sentence_without_sw = sentence# creation d'une liste de phrase tmp\n",
    "        for word in sentence:\n",
    "            #word = nltk.pos_tag(wnl.lemmatize(word, pos='v'))\n",
    "            for sw in stop_words:\n",
    "                if word==sw or word==\".\" or word==\":\" or word==\",\" or word==\";\" or word==')' or word =='(':\n",
    "                    if word in sentence_without_sw:\n",
    "                        sentence_without_sw.remove(word)\n",
    "        sentence = sentence_without_sw #permet de retirer les stop word sans changer la longueur de la liste\n",
    "fin = time.time()\n",
    "print(\"Le temps écoulé pour ce traitement a prit : \", fin-debut)\n",
    "display(dfc[0][1]) \n",
    "#1 ere ligne bien parsé mais pas taggé => ['obviously', 'made', 'show', 'famous', '1950s', 'stripper',\n",
    "#'misty', 'ayers', '``', 'acting', \"''\", 'talents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Création d'une liste de StopWords et mots à supprimer de tous nos commentaires car inutiles !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stop_words2 = set([\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tag des mots & chunking & chanking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "L'ajout de tag aux mots est très important car il permet de leur donner un certain poid lors de l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk import pos_tag\n",
    "token = []\n",
    "for i in range(0, len(df)-1, 1):\n",
    "    for j in range(0, len(dfc.loc[i])-1, 1):\n",
    "        \n",
    "        token = pos_tag(dfc_phrases[i][j])\n",
    "        dfc_phrases[i][j] = token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('obviously', 'RB'),\n",
       " ('made', 'VBD'),\n",
       " ('show', 'NN'),\n",
       " ('famous', 'JJ'),\n",
       " ('1950s', 'CD'),\n",
       " ('stripper', 'NN'),\n",
       " ('misty', 'NN'),\n",
       " ('ayers', 'NNS'),\n",
       " ('``', '``'),\n",
       " ('acting', 'VBG'),\n",
       " (\"''\", \"''\"),\n",
       " ('talents', 'NNS')]"
      ]
     },
     "execution_count": 312,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dfc_phrases[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  unless/IN\n",
      "  are/VBP\n",
      "  already/RB\n",
      "  familiar/JJ\n",
      "  (Chunk pop/NN)\n",
      "  (Chunk stars/NNS)\n",
      "  (Chunk star/VBP film/NN)\n",
      "  save/VB\n",
      "  the/DT\n",
      "  (Chunk time/NN)\n",
      "  (Chunk stop/VB reading/VBG review/NN)\n",
      "  've/VBP\n",
      "  reached/VBN\n",
      "  the/DT\n",
      "  (Chunk end/NN)\n",
      "  the/DT\n",
      "  next/JJ\n",
      "  (Chunk sentence.forget/NN)\n",
      "  you/PRP\n",
      "  ever/RB\n",
      "  stumbled/VBD\n",
      "  upon/IN\n",
      "  this/DT\n",
      "  (Chunk film/NN)\n",
      "  (Chunk go/VB watch/NN)\n",
      "  (Chunk something/NN)\n",
      "  (Chunk else.but/NN)\n",
      "  you/PRP\n",
      "  (Chunk insist/VBP reading/VBG consider/NN)\n",
      "  lame/JJ\n",
      "  (Chunk vehicle/NN)\n",
      "  japanese/JJ\n",
      "  teen/JJ\n",
      "  idol/JJ\n",
      "  pretty-boys/JJ\n",
      "  featuring/VBG\n",
      "  nonsensical/JJ\n",
      "  convoluted/VBN\n",
      "  ``/``\n",
      "  (Chunk plot/NN)\n",
      "  ''/''\n",
      "  (Chunk drags/NNS)\n",
      "  for/IN\n",
      "  insufferable/JJ\n",
      "  (Chunk amount/NN)\n",
      "  (Chunk time/NN)\n",
      "  you/PRP\n",
      "  're/VBP\n",
      "  ready/JJ\n",
      "  scream.nothing/VBG\n",
      "  this/DT\n",
      "  (Chunk film/NN)\n",
      "  (Chunk makes/VBZ sense/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "#Quelques tests de regex\n",
    "\n",
    "PHRASE = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "VBNN = r\"\"\"Chunk: {<VB.?>*<NN.?>}\"\"\"\n",
    "NN = r\"\"\"Chunk : {<NN.>} \"\"\" # Noms\n",
    "JJ = r\"\"\"Chunk : {<JJ.?>}\"\"\" # Adjectifs\n",
    "\n",
    "ADV = r\"\"\"{<W.*?>}\"\"\" # Adverbes\n",
    "\n",
    "\n",
    "chunkParser = nltk.RegexpParser(VBNN)\n",
    "\n",
    "chunked = chunkParser.parse(dfc_phrases[2][0])\n",
    "print(chunked)\n",
    "#chunked.draw()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Retrait des symboles et conjonctions de coordinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk around/IN)\n",
      "  time/NN\n",
      "  europe/NN\n",
      "  began/VBD\n",
      "  fighting/VBG\n",
      "  world/NN\n",
      "  war/NN\n",
      "  ii/NN\n",
      "  (Chunk until/IN)\n",
      "  war/NN\n",
      "  's/POS\n",
      "  end/NN\n",
      "  hollywood/NN\n",
      "  (Chunk with/IN)\n",
      "  significant/JJ\n",
      "  prodding/VBG\n",
      "  government/NN\n",
      "  made/VBD\n",
      "  tons/NNS\n",
      "  movies/NNS\n",
      "  were/VBD\n",
      "  designed/VBN\n",
      "  try/NN\n",
      "  get/VB\n",
      "  young/JJ\n",
      "  men/NNS\n",
      "  enlist/VBP\n",
      "  the/DT\n",
      "  army/NN\n",
      "  making/VBG\n",
      "  the/DT\n",
      "  life/NN\n",
      "  a/DT\n",
      "  serviceman/JJ\n",
      "  appear/VBP\n",
      "  ``/``\n",
      "  cool/JJ\n",
      "  ''/'')\n"
     ]
    }
   ],
   "source": [
    "TRASH = r\"\"\"Chunk : {<SYM|CC|IN>+}\"\"\"\n",
    "\n",
    "TRASHREVERSE = r\"\"\"Chunk : {<.*>+}\n",
    "                        }<SYM|CC|IN>+{\"\"\" # Anything + symboles ou CC \n",
    "\n",
    "\n",
    "# Inverse de Chuck, on séléctionne tout sauf ce qui est précisé.\n",
    "\n",
    "chunkParser = nltk.RegexpParser(TRASH)\n",
    "\n",
    "chunked = chunkParser.parse(dfc_phrases[3][0])\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sentence_name = dfc_phrases.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "VB_AVOIR_ETRE = [ 'VB','VBD','VBG','VBN','VBZ','VBP','VD','VDD','VDG','VDN','VDZ','VDP','VH','VHD','VHG','VHN','VHZ','VHP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TO do : Faire des listes pour tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Before'"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('obviously', 'RB'),\n",
       " ('made', 'VBD'),\n",
       " ('show', 'NN'),\n",
       " ('famous', 'JJ'),\n",
       " ('1950s', 'CD'),\n",
       " ('stripper', 'NN'),\n",
       " ('misty', 'NN'),\n",
       " ('ayers', 'NNS'),\n",
       " ('``', '``'),\n",
       " ('acting', 'VBG'),\n",
       " (\"''\", \"''\"),\n",
       " ('talents', 'NNS')]"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('film', 'NN'),\n",
       " ('more', 'RBR'),\n",
       " ('effective', 'JJ'),\n",
       " ('persuading', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('zionist', 'NN'),\n",
       " ('conspiracy', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('muslim', 'NN'),\n",
       " ('one', 'CD')]"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('was', 'VBD'),\n",
       " ('oliver', 'RB'),\n",
       " ('stone', 'JJ'),\n",
       " ('these', 'DT'),\n",
       " ('years', 'NNS'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'After'"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['show', 'famous', 'stripper', 'misty', 'ayers', '``', \"''\", 'talents']"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['film', 'more', 'effective', 'persuading', 'zionist', 'conspiracy', 'muslim']"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['stone', 'years', '?']"
      ]
     },
     "execution_count": 317,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "display(\"Before\")\n",
    "display(sentence_name[0][0])\n",
    "display(sentence_name[1][0])\n",
    "display(sentence_name[5][5])\n",
    "\n",
    "for i in range(0, len(df)-1, 1):\n",
    "    for j in range(0, len(dfc.loc[i])-1, 1):\n",
    "            #word = nltk.pos_tag(wnl.lemmatize(word, pos='v'))\n",
    "        sentence_name[i][j] = ([word for word,pos in dfc_phrases[i][j]\n",
    "            if (((pos != 'WDT') and (pos != 'WP') and (pos != 'WP$') and \\\n",
    "                 (pos != 'WRB') and (pos != ':') and (pos != '$' ) and \\\n",
    "                 (pos != 'TO') and (pos != 'SYM') and (pos != 'RB') and \\\n",
    "                 (pos != 'PP') and (pos != 'PP$') and (pos != 'POS') and \\\n",
    "                 (pos != 'PDT') and (pos != 'MD') and (pos != 'LS') and \\\n",
    "                 (pos != 'FW') and (pos != 'IN') and (pos != 'EX' )and \\\n",
    "                 (pos != 'DT') and (pos != 'CD') and (pos != 'C') )and (pos not in VB_AVOIR_ETRE) )]) #\n",
    "\n",
    "display(\"After\")\n",
    "display(sentence_name[0][0])\n",
    "display(sentence_name[1][0])\n",
    "display(sentence_name[5][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Prendre en concidération les occurences rares ou celles apparaissants trop souvent : les virer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Relation entre les mots : N-Grams tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-377dc13fcba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigram_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigramTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#bigram_tagger.tag(train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bigram_tagger = nltk.BigramTagger(train)\n",
    "#bigram_tagger.tag(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Une fois que nos data ont subit tous les pré-traitements, créer un nouveau fichier avec les données néttoyées pour permettre l'application de bag of Words. </br>\n",
    "Peut être que les bag of words ne sont à appliquer que sur les mots les plus significatifs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "with open (\"./Data/dataset.csv\", \"r\") as file:\n",
    "    X_train_counts = count_vect.fit_transform(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "display(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "display(X_train_counts.todense() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}